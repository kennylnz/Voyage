<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Dockerize Spark 2.3.2 + Hadoop 2.7.1 | Voyage</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Today let’s talk about how to dockerize Spark, a Lightning-fast unified analytics engine.If you search official image hub for Spark, you will find following12345NAME">
<meta property="og:type" content="article">
<meta property="og:title" content="Dockerize Spark 2.3.2 + Hadoop 2.7.1">
<meta property="og:url" content="https://kennylnz.github.io/2018/10/13/Dockerize-Spark-2-3-2-Hadoop-2-7-1/index.html">
<meta property="og:site_name" content="Voyage">
<meta property="og:description" content="Today let’s talk about how to dockerize Spark, a Lightning-fast unified analytics engine.If you search official image hub for Spark, you will find following12345NAME">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2018-10-13T04:19:22.638Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Dockerize Spark 2.3.2 + Hadoop 2.7.1">
<meta name="twitter:description" content="Today let’s talk about how to dockerize Spark, a Lightning-fast unified analytics engine.If you search official image hub for Spark, you will find following12345NAME">
  
    <link rel="alternate" href="/Voyage/atom.xml" title="Voyage" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/Voyage/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/Voyage/" id="logo">Voyage</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/Voyage/">Home</a>
        
          <a class="main-nav-link" href="/Voyage/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/Voyage/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://kennylnz.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Dockerize-Spark-2-3-2-Hadoop-2-7-1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/Voyage/2018/10/13/Dockerize-Spark-2-3-2-Hadoop-2-7-1/" class="article-date">
  <time datetime="2018-10-13T01:25:32.000Z" itemprop="datePublished">2018-10-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Dockerize Spark 2.3.2 + Hadoop 2.7.1
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Today let’s talk about how to dockerize Spark, a Lightning-fast unified analytics engine.<br>If you search official image hub for Spark, you will find following<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">NAME                                   DESCRIPTION                                     STARS               OFFICIAL            AUTOMATED</span><br><span class="line">sequenceiq/spark                       An easy way to try Spark                        415                                     [OK]</span><br><span class="line">jupyter/all-spark-notebook             Jupyter Notebook Python, Scala, R, Spark, Me…   200</span><br><span class="line">gettyimages/spark                      A debian:jessie based Spark container           103                                     [OK]</span><br><span class="line">mesosphere/spark                       DCOS Spark                                      86</span><br></pre></td></tr></table></figure></p>
<p>Let’s just start with most popular ones, sequenceiq/spark. But you will find from their <a href="https://hub.docker.com/r/sequenceiq/spark/" target="_blank" rel="noopener">website</a>, is for spark 1.6. That’s too old.<br>Let’s see whether we can build image for latest spark based on some of their work. The plan is get sequenceiq/spark’s docker file and make updates to their articfacts to install latest spark</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/sequenceiq/docker-spark</span><br></pre></td></tr></table></figure>
<p>In the Dockerfile<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">FROM sequenceiq/hadoop-docker:2.6.0</span><br><span class="line">MAINTAINER SequenceIQ</span><br><span class="line"></span><br><span class="line">#support for Hadoop 2.6.0</span><br><span class="line">RUN curl -s http://d3kbcqa49mib13.cloudfront.net/spark-1.6.1-bin-hadoop2.6.tgz | tar -xz -C /usr/local/</span><br><span class="line">RUN cd /usr/local &amp;&amp; ln -s spark-1.6.1-bin-hadoop2.6 spark</span><br><span class="line">ENV SPARK_HOME /usr/local/spark</span><br><span class="line">RUN mkdir $SPARK_HOME/yarn-remote-client</span><br><span class="line">ADD yarn-remote-client $SPARK_HOME/yarn-remote-client</span><br><span class="line"></span><br><span class="line">RUN $BOOTSTRAP &amp;&amp; $HADOOP_PREFIX/bin/hadoop dfsadmin -safemode leave &amp;&amp; $HADOOP_PREFIX/bin/hdfs dfs -put $SPARK_HOME-1.6.1-bin-hadoop2.6/lib /spark</span><br><span class="line"></span><br><span class="line">ENV YARN_CONF_DIR $HADOOP_PREFIX/etc/hadoop</span><br><span class="line">ENV PATH $PATH:$SPARK_HOME/bin:$HADOOP_PREFIX/bin</span><br><span class="line"># update boot script</span><br><span class="line">COPY bootstrap.sh /etc/bootstrap.sh</span><br><span class="line">RUN chown root.root /etc/bootstrap.sh</span><br><span class="line">RUN chmod 700 /etc/bootstrap.sh</span><br><span class="line"></span><br><span class="line">#install R</span><br><span class="line">RUN rpm -ivh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm</span><br><span class="line">RUN yum -y install R</span><br><span class="line"></span><br><span class="line">ENTRYPOINT [&quot;/etc/bootstrap.sh&quot;]</span><br></pre></td></tr></table></figure></p>
<p>Go thru the file quickly, It’s based on hadoop 2.6 image, also from sequenceiq. It downloads the spark 1.6.1, and put spark files to hadoop server. so here is out plan of updating<br>1 use latest hadoop image from <a href="https://github.com/sequenceiq/hadoop-docker" target="_blank" rel="noopener">sequenceiq</a>, which is 2.7.1<br>2 download latest spark on hadoop 2.7.1 which is <a href="http://spark.apache.org/downloads.html" target="_blank" rel="noopener">2.3.2on hadoop 2.7.1 </a>.<br>3 update bootstrap(from the sequenceiq git project) file for starting hadoop/spark.<br>4 spark 2.3.2 requires jdk1.8, but hadoop image we got is using jdk 1.7,  luckily, when installing the R, jdk1.8 was installed. so we need switch the link of java to point to jdk1.8<br> which is installed at /usr/lib/jvm/jre-1.8.0-openjdk.x86_64/ (the hadoop-env in sequenceiq’s image using /usr/java/default for JAVA_HOME )<br>5 it turns out the sequenceiq image will run into memory issue, I have to tune the yarn memory setting couple of times.</p>
<p>yarn-site.xml under the yarn-remote-client, add following properties. there is some redundant between them you can google it for more percise setting according to your environment.<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether virtual memory limits will be enforced for containers.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether physical memory limits will be enforced for containers.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span>   </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>4<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether physical memory limits will be enforced for containers.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>New Dockerfile, adjusted the order of steps to put slow steps in the front(downloading, installing) so it wil be faster if you rebuild image when Dockerfile adjusted.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#support for Hadoop 2.7.1</span></span><br><span class="line">FROM sequenceiq/hadoop-docker:2.7.1</span><br><span class="line">MAINTAINER Voyage</span><br><span class="line"></span><br><span class="line">RUN curl -s https://archive.apache.org/dist/spark/spark-2.3.2/spark-2.3.2-bin-hadoop2.7.tgz | tar -xz -C /usr/<span class="built_in">local</span>/</span><br><span class="line">RUN <span class="built_in">cd</span> /usr/<span class="built_in">local</span> &amp;&amp; ln -s spark-2.3.2-bin-hadoop2.7 spark</span><br><span class="line">ENV SPARK_HOME /usr/<span class="built_in">local</span>/spark</span><br><span class="line">RUN mkdir <span class="variable">$SPARK_HOME</span>/yarn-remote-client</span><br><span class="line">ADD yarn-remote-client <span class="variable">$SPARK_HOME</span>/yarn-remote-client</span><br><span class="line"></span><br><span class="line"><span class="comment">#install R</span></span><br><span class="line">RUN rpm -ivh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm</span><br><span class="line">RUN yum -y install R</span><br><span class="line"></span><br><span class="line"><span class="comment">#use jdk 1.8</span></span><br><span class="line">RUN rm /usr/java/default</span><br><span class="line">RUN ln -s /usr/lib/jvm/jre-1.8.0-openjdk.x86_64/ /usr/java/default</span><br><span class="line"></span><br><span class="line"><span class="comment">#put spark lib jars into hadoop</span></span><br><span class="line">RUN <span class="variable">$BOOTSTRAP</span> &amp;&amp; <span class="variable">$HADOOP_PREFIX</span>/bin/hadoop dfsadmin -safemode leave &amp;&amp; <span class="variable">$HADOOP_PREFIX</span>/bin/hdfs dfs -put <span class="variable">$SPARK_HOME</span>/jars /spark</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ENV YARN_CONF_DIR <span class="variable">$HADOOP_PREFIX</span>/etc/hadoop</span><br><span class="line">ENV PATH <span class="variable">$PATH</span>:<span class="variable">$SPARK_HOME</span>/bin:<span class="variable">$HADOOP_PREFIX</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="comment"># update boot script</span></span><br><span class="line">COPY bootstrap.sh /etc/bootstrap.sh</span><br><span class="line">RUN chown root.root /etc/bootstrap.sh</span><br><span class="line">RUN chmod 700 /etc/bootstrap.sh </span><br><span class="line"></span><br><span class="line"><span class="comment"># update yarn config</span></span><br><span class="line">RUN mv <span class="variable">$HADOOP_PREFIX</span>/etc/hadoop/yarn-site.xml <span class="variable">$HADOOP_PREFIX</span>/etc/hadoop/yarn-site.xml.bk</span><br><span class="line">RUN cp <span class="variable">$SPARK_HOME</span>/yarn-remote-client/yarn-site.xml <span class="variable">$HADOOP_PREFIX</span>/etc/hadoop/yarn-site.xml</span><br><span class="line"></span><br><span class="line">ENTRYPOINT [<span class="string">"/etc/bootstrap.sh"</span>]</span><br></pre></td></tr></table></figure></p>
<p>minor update to the bootstrap.sh<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">: <span class="variable">$&#123;HADOOP_PREFIX:=/usr/local/hadoop&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="variable">$HADOOP_PREFIX</span>/etc/hadoop/hadoop-env.sh</span><br><span class="line"></span><br><span class="line">rm /tmp/*.pid</span><br><span class="line"></span><br><span class="line"><span class="comment"># installing libraries if any - (resource urls added comma separated to the ACP system variable)</span></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$HADOOP_PREFIX</span>/share/hadoop/common ; <span class="keyword">for</span> cp <span class="keyword">in</span> <span class="variable">$&#123;ACP//,/ &#125;</span>; <span class="keyword">do</span>  <span class="built_in">echo</span> == <span class="variable">$cp</span>; curl -LO <span class="variable">$cp</span> ; <span class="keyword">done</span>; <span class="built_in">cd</span> -</span><br><span class="line"></span><br><span class="line"><span class="comment"># altering the core-site configuration</span></span><br><span class="line">sed s/HOSTNAME/<span class="variable">$HOSTNAME</span>/ /usr/<span class="built_in">local</span>/hadoop/etc/hadoop/core-site.xml.template &gt; /usr/<span class="built_in">local</span>/hadoop/etc/hadoop/core-site.xml</span><br><span class="line"></span><br><span class="line"><span class="comment"># setting spark defaults</span></span><br><span class="line"><span class="built_in">echo</span> spark.yarn.jars hdfs:///spark/* &gt; <span class="variable">$SPARK_HOME</span>/conf/spark-defaults.conf</span><br><span class="line">cp <span class="variable">$SPARK_HOME</span>/conf/metrics.properties.template <span class="variable">$SPARK_HOME</span>/conf/metrics.properties</span><br><span class="line"></span><br><span class="line">service sshd start</span><br><span class="line"><span class="variable">$HADOOP_PREFIX</span>/sbin/start-dfs.sh</span><br><span class="line"><span class="variable">$HADOOP_PREFIX</span>/sbin/start-yarn.sh</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">CMD=<span class="variable">$&#123;1:-"exit 0"&#125;</span></span><br><span class="line"><span class="keyword">if</span> [[ <span class="string">"<span class="variable">$CMD</span>"</span> == <span class="string">"-d"</span> ]];</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">	service sshd stop</span><br><span class="line">	/usr/sbin/sshd -D -d</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">	/bin/bash -c <span class="string">"$*"</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure></p>
<p>bulid the image， tag it with voyage/spark, version 2.3.2<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build --rm -t voyage/spark:2.3.2 .</span><br></pre></td></tr></table></figure></p>
<p>the first time might be slow as you need pull the hadoop image.</p>
<p>run a container<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -it -p 8088:8088 -p 8042:8042 -p 4040:4040 -h sandbox voyage/spark:2.3.2 bash</span><br></pre></td></tr></table></figure></p>
<p>Run the yarn client to verify, within the container. please ensure you have enough memory for your container(container os, hadoop &amp; spark)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash-4.1<span class="comment"># spark-shell --master yarn --deploy-mode client --driver-memory 1g --executor-memory 1g --executor-cores 1</span></span><br></pre></td></tr></table></figure></p>
<p>you should be able to see<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">....</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  <span class="string">'_/</span></span><br><span class="line"><span class="string">   /___/ .__/\_,_/_/ /_/\_\   version 2.3.2</span></span><br><span class="line"><span class="string">      /_/</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Using Scala version 2.11.8 (OpenJDK 64-Bit Server VM, Java 1.8.0_181)</span></span><br><span class="line"><span class="string">Type in expressions to have them evaluated.</span></span><br><span class="line"><span class="string">Type :help for more information.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">scala&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>now you can run some scala script</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.parallelize(<span class="number">1</span> to <span class="number">1000</span>).count()</span><br><span class="line">res0: <span class="type">Long</span> = <span class="number">1000</span>                                                      </span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kennylnz.github.io/2018/10/13/Dockerize-Spark-2-3-2-Hadoop-2-7-1/" data-id="cjnh1izse0004sgvojouluecp" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/Voyage/2018/10/14/Access-DynamoDB-from-Browser/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Access DynamoDB from Browser
        
      </div>
    </a>
  
  
    <a href="/Voyage/2018/10/11/Dockerize-Mongo-and-Nodejs/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Dockerize Mongo and Nodejs</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/Voyage/archives/2018/10/">October 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/Voyage/2018/10/20/Docker-Notes/">Docker Notes</a>
          </li>
        
          <li>
            <a href="/Voyage/2018/10/19/Docker-Summarize/">Docker Summarize</a>
          </li>
        
          <li>
            <a href="/Voyage/2018/10/14/Access-DynamoDB-from-Browser/">Access DynamoDB from Browser</a>
          </li>
        
          <li>
            <a href="/Voyage/2018/10/13/Dockerize-Spark-2-3-2-Hadoop-2-7-1/">Dockerize Spark 2.3.2 + Hadoop 2.7.1</a>
          </li>
        
          <li>
            <a href="/Voyage/2018/10/11/Dockerize-Mongo-and-Nodejs/">Dockerize Mongo and Nodejs</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 Kenny Liu<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/Voyage/" class="mobile-nav-link">Home</a>
  
    <a href="/Voyage/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/Voyage/fancybox/jquery.fancybox.css">
  <script src="/Voyage/fancybox/jquery.fancybox.pack.js"></script>


<script src="/Voyage/js/script.js"></script>



  </div>
</body>
</html>